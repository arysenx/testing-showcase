# Testing Showcase Framework

[![CI/CD Status](https://github.com/arysenx/testing-showcase/actions/workflows/automated-testing.yaml/badge.svg)](https://github.com/username/testing-showcase/actions)
[![Python 3.11](https://img.shields.io/badge/Python-3.11-blue?logo=python&logoColor=white)](https://python.org)
[![Pytest](https://img.shields.io/badge/Pytest-8.0+-green?logo=pytest&logoColor=white)](https://docs.pytest.org/)
[![Selenium](https://img.shields.io/badge/Selenium-4.16-43B02A?logo=selenium&logoColor=white)](https://www.selenium.dev/)
[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

---

## ðŸŽ¯ Project Objective

This framework was engineered to demonstrate a **modern, scalable approach to Test Automation** that bridges the gap between fast API validation and user-centric UI testing. It addresses common challenges such as flaky tests, maintenance overhead, and fragmented reporting.

### Key Architectural Decisions
- **Page Object Model (POM)**: Strict encapsulation of UI elements and interactions to decouple test logic from DOM implementation details.
- **Config-Driven**: Environment-agnostic design using `config.yml` helps easily switch between Dev, Staging, and Prod environments.
- **CI/CD First**: Native integration with GitHub Actions including **headless execution** and artifact management.

---

## ðŸ› ï¸ Technology Stack

| Category | Technology | Justification |
| :--- | :--- | :--- |
| **Core Framework** | **Pytest 8.x** |
| **API Engine** | **Requests** |
| **UI Engine** | **Selenium 4** |
| **Driver Mgmt** | **WebDriver Manager** |
| **Validation** | **JSONSchema** |
| **Reporting** | **Pytest-HTML** |
| **CI/CD** | **GitHub Actions** |

---

## ðŸ“ Architecture

```text
testing-showcase/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ automated-testing.yaml  # CI/CD Pipeline definition
â”œâ”€â”€ reports/                        # Test artifacts (HTML reports)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ api/                        # API Test Suite
â”‚   â”‚   â”œâ”€â”€ conftest.py             # API-specific fixtures
â”‚   â”‚   â””â”€â”€ test_pokemon.py         # API test implementation
â”‚   â”œâ”€â”€ ui/                         # UI Test Suite
â”‚   â”‚   â”œâ”€â”€ conftest.py             # WebDriver & Page Loader fixtures
â”‚   â”‚   â””â”€â”€ test_login.py           # UI test implementation
â”‚   â”œâ”€â”€ pages/                      # Page Object Models (POM)
â”‚   â”‚   â”œâ”€â”€ base.py                 # BasePage with explicit waits
â”‚   â”‚   â””â”€â”€ login.py                # LoginPage logic
â”‚   â”œâ”€â”€ common/                     # Shared Utilities
â”‚   â”‚   â”œâ”€â”€ schemas/                # JSON Schemas
â”‚   â”‚   â””â”€â”€ helpers.py              # Config parsers & tools
â”‚   â””â”€â”€ conftest.py                 # Global hooks & configuration
â”œâ”€â”€ config.yml                      # Centralized Configuration
â”œâ”€â”€ pytest.ini                      # Pytest runner configuration
â””â”€â”€ requirements.txt                # Dependency definitions
```

### Implemented Patterns
- **Page Object Model**: `tests/pages/`
- **Fixture Injection**: Extensive use of `conftest.py` for DI.
- **Strategy Pattern**: `page_loader` fixture for dynamic page instantiation.
- **Data-Driven Testing**: Parametrization in tests.

---

## ðŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Google Chrome (for UI tests)

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/username/testing-showcase.git
cd testing-showcase

# 2. Create virtual environment
python -m venv venv
# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate

# 3. Install dependencies
pip install -r tests/requirements.txt
```

### Execution

**Run All Tests:**
```bash
python -m pytest tests/ -v
```

**Run Only API Tests:**
```bash
python -m pytest tests/api -v
```

**Run UI Tests (Headless):**
```bash
python -m pytest tests/ui --headless -v
```

**Generate HTML Report:**
```bash
python -m pytest tests/ --html=reports/report.html
```

**Run Behave Tests:**
```bash
behave bdd/features
```

---

## â­ Key Features

### ðŸ”„ Intelligent Headless Mode
The framework automatically supports headless execution for CI environments via custom CLI flags.
```python
# tests/ui/conftest.py
if request.config.getoption("--headless"):
    chrome_options.add_argument("--headless")
```

### ðŸ” Multi-Environment Configuration
Uses `config.yml` to separate test data from code, allowing secure and flexible configuration.
```yaml
# config.yml
environments:
  staging:
    base_url: https://staging.example.com
```

### ðŸ“‰ Robust Reporting
Generates detailed HTML reports including error traces, duration, and status.
![Report Example]()

---

## ðŸ”„ CI/CD Pipeline

The project uses GitHub Actions for continuous integration.

- **Trigger**: Pushes and Pull Requests to `main`.
- **Environment**: Ubuntu Latest + Python 3.11.
- **Steps**:
    1. Checkout Code.
    2. Install Dependencies (`pip install`).
    3. Run Tests in **Headless Mode**.
    4. Upload HTML Report as Artifact.

---

## ðŸ“ˆ Project Metrics

| Metric | Value |
| :--- | :--- |
| **Test Coverage** | API + UI |
| **Execution Time** | ~15s (local) |
| **Stability** | 100% Pass Rate |
| **Maintainability** | High (Modular Design) |

---

## ðŸ¤ Contact

**Portfolio Candidate** - QA Automation Engineer | SDET
[LinkedIn](https://www.linkedin.com/in/angel-ramiro-jimenez/) | [Email](mailto:arj96@outlook.es)

> **Open to opportunities for**: SDET, ADET, QA Automation Engineer.

---
